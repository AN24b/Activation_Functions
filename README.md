# Activation_Functions
Here's a one-line description of each activation function:

Sigmoid: Converts inputs into a value between 0 and 1, useful for binary classification.

ReLU: Outputs the input if positive, otherwise 0, commonly used in deep networks.

Tanh: Scales inputs between -1 and 1, centering around zero for faster learning.

Softmax: Transforms logits into probabilities for multi-class classification.

I'm implementing these manually without using inbuilt libraries.
